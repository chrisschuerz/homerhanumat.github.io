<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Student-Focused Tutorials | A Statistics Blog]]></title>
  <link href="http://homerhanumat.github.io/blog/categories/student-focused-tutorials/atom.xml" rel="self"/>
  <link href="http://homerhanumat.github.io/"/>
  <updated>2014-02-21T09:48:50-05:00</updated>
  <id>http://homerhanumat.github.io/</id>
  <author>
    <name><![CDATA[Homer White]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tutorial on Mosaic's Favstats()]]></title>
    <link href="http://homerhanumat.github.io/blog/2014/02/21/favstatstutorial/"/>
    <updated>2014-02-21T15:00:00-05:00</updated>
    <id>http://homerhanumat.github.io/blog/2014/02/21/favstatstutorial</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#studying-one-numerical-variable">Studying One Numerical Variable</a></li>
  <li><a href="#studying-the-relationship-between-a-numerical-variable-and-a-factor-variable">Studying the Relationship Between a Numerical Variable and a Factor Variable</a></li>
  <li><a href="#limiting-the-output">Limiting the Output</a></li>
  <li><a href="#warning">Warning</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p><code>favstats()</code> comes from the <code>mosaic</code> package and we will use some data from the <code>tigerstats</code> package, so make sure that both are loaded:</p>

<pre><code>require(mosaic)
require(tigerstats)
</code></pre>

<p><strong>Note:</strong>  If you are not working with the R Studio server hosted by Georgetown College, then you will need install <code>tigerstats</code> on your own machine.  You can get the current version from <a href="http://github.com">Github</a> by first installing the <code>devtools</code> package from the CRAN repository, and then running the following commands in a fresh R session:</p>

<pre><code>require(devtools)
install_github(repo = "homerhanumat/tigerstats")
</code></pre>

<p>In this tutorial we will work with the <code>m111survey</code> data frame from <code>tigerstats</code> package.  If you are not yet familiar with this data, then run:</p>

<pre><code>data(m111survey)
View(m111survey)
help(m111survey)
</code></pre>

<p>Remember that you can also learn about the types of each variable in the data frame with the <code>str()</code> function:</p>

<pre><code>str(m111survey)

## 'data.frame':	71 obs. of  12 variables:
##  $ height         : num  76 74 64 62 72 70.8 70 79 59 67 ...
##  $ ideal_ht       : num  78 76 NA 65 72 NA 72 76 61 67 ...
##  $ sleep          : num  9.5 7 9 7 8 10 4 6 7 7 ...
##  $ fastest        : int  119 110 85 100 95 100 85 160 90 90 ...
##  $ weight_feel    : Factor w/ 3 levels "1_underweight",..: 1 2 2 1 1 3 2 2 2 3 ...
##  $ love_first     : Factor w/ 2 levels "no","yes": 1 1 1 1 1 1 1 1 1 1 ...
##  $ extra_life     : Factor w/ 2 levels "no","yes": 2 2 1 1 2 1 2 2 2 1 ...
##  $ seat           : Factor w/ 3 levels "1_front","2_middle",..: 1 2 2 1 3 1 1 3 3 2 ...
##  $ GPA            : num  3.56 2.5 3.8 3.5 3.2 3.1 3.68 2.7 2.8 NA ...
##  $ enough_Sleep   : Factor w/ 2 levels "no","yes": 1 1 1 1 1 2 1 2 1 2 ...
##  $ sex            : Factor w/ 2 levels "female","male": 2 2 1 1 2 2 2 2 1 1 ...
##  $ diff.ideal.act.: num  2 2 NA 3 0 NA 2 -3 2 0 ...
</code></pre>

<h2 id="studying-one-numerical-variable">Studying One Numerical Variable</h2>

<p>“favstats” is short for “favorite statistics”:  it will give you the some of the most popular summary statistics for numerical variables.</p>

<p>Suppose, for example, that you want to know how fast people in the <code>m111survey</code> sample tend to drive, when they drive their fastest. The you want to study the numerical variable <strong>fastest</strong>:  the fastest speed each person claims to have ever driven, measured in miles per hour.  Just try <code>favstats()</code> with the usual formula-data input:</p>

<pre><code>favstats(~fastest, data = m111survey)

##  min   Q1 median    Q3 max  mean    sd  n missing
##   60 90.5    102 119.5 190 105.9 20.88 71       0
</code></pre>

<p>Remember what each of the statistics tells you:</p>

<ul>
  <li>The minimum fastest speed drive by anyone in the survery was 60 mph.</li>
  <li>The maximum fastest speed was 190 mph.</li>
  <li>About 25% of the survey participants drove less than 90.5 mph (the First Quartile)</li>
  <li>About 75% percent of the survey participants drove less than 119.5 mph (the Third Quartile)</li>
  <li>About 50% of the participants drove less than 102 mph (the median)</li>
  <li>The mean speed for this sample of students was about 105.9 mph …</li>
  <li>… give or take about 20.9 mph or so (the standard deviation).</li>
</ul>

<p>We also see that</p>

<ul>
  <li>71 people answered the question about fastest speed;</li>
  <li>Nobody did not answer (missing = 0).</li>
</ul>

<h2 id="studying-the-relationship-between-a-numerical-variable-and-a-factor-variable">Studying the Relationship Between a Numerical Variable and a Factor Variable</h2>

<p>Studying the relationship between a numerical variable and factor variable involves what is popularly known as “breaking the data into groups” based on the values of the factor variable.  More formally, we obtain the conditional distributions of the numerical variable given the various possible values of the factor variable, and look for difference between these distributions.  If we see large differences. then we know that the factor variable “makes a difference” in the likely values of the numerical variable, i.e., the two variable are related.</p>

<p>For example we might want to know if the fastest speed one drives might be related to one’s sex.  The relevant variables in <code>m111survey</code> are then the numerical <strong>fastest</strong> and the factor variable <strong>sex</strong>.</p>

<p>In formula-data input for <code>favstats()</code> the formula always follows the format:</p>

<script type="math/tex; mode=display">numerical \sim factor.</script>

<p>So we run the following command:</p>

<pre><code>favstats(fastest ~ sex, data = m111survey)

##        min Q1 median    Q3 max  mean    sd  n missing
## female  60 90     95 110.0 145 100.0 17.61 40       0
## male    85 99    110 122.5 190 113.5 22.57 31       0
</code></pre>

<p>The first row of the output gives a summary of the conditional distribution of <strong>fastest</strong>, given that <strong>sex</strong> is female.</p>

<p>The second row summarizes the conditional distribution of <strong>fastest</strong>, given that <strong>sex</strong> is male.</p>

<p>The two conditional distribution are not the same.  For example, we see that on average females drove about 100 mph, whereas the guys drove about 113.4 mph.  The guys appear to drive faster than the gals:  for this sample of students, fastest speed drive does indeed appear to be related to sex.</p>

<h2 id="limiting-the-output">Limiting the Output</h2>

<p>Sometimes you want just a few of the numbers from <code>favstats()</code>.  If you would like to display only those numbers you can do so using brackets “[” and “]”, along with a list of the names of the columns you want to see.  For example, to display only the means and the standard deviations for <strong>fastest</strong> broken down by <strong>sex</strong>, ask for:</p>

<pre><code>favstats(fastest ~ sex, data = m111survey)[c("mean", "sd")]

##         mean    sd
## female 100.0 17.61
## male   113.5 22.57
</code></pre>

<p>The brackets are R’s way of locating particular parts of an object.  If you want to display more than one column, make sure to combine their names (in quotes) in a list with the <code>c()</code> function, as shown above.</p>

<h2 id="warning">Warning</h2>

<p><code>favstats()</code> specializes in numerical variables:  it won’t help you to study a factor variable by itself.  Look what happens when you try to use it to study the factor variable <strong>sex</strong>:</p>

<pre><code>favstats(~sex, data = m111survey)

## Error: non-numeric argument to binary operator
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[lmGC() Tutorial]]></title>
    <link href="http://homerhanumat.github.io/blog/2014/02/15/lmGCtutorial/"/>
    <updated>2014-02-15T15:00:00-05:00</updated>
    <id>http://homerhanumat.github.io/blog/2014/02/15/lmGCtutorial</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#formula-data-input">Formula-Data Input</a></li>
  <li><a href="#prediction">Prediction</a></li>
  <li><a href="#plotting">Plotting</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p>The <code>lmGC()</code> is a starter-tool for simple linear regression, when you are studying the relationship between two numerical variables, one of which you consider to be an explanatory or predictor variable and the other of which you think of as the response.  It’s only a starter-tool:  by the end of the course, or in later statistics courses, you will move on to R’s function <code>lm()</code>, which allows you to work with more than one explanatory variable and which provides additional useful information in its output.  Also, in <code>lm()</code> the explanatory variable(s) can even be factors!</p>

<p>The function (and some of the data) that we will use comes from the <code>tigerstats</code> package, so make sure that it is loaded:</p>

<pre><code>require(tigerstats)
</code></pre>

<p><strong>Note:</strong>  If you are not working with the R Studio server hosted by Georgetown College, then you will need install <code>tigerstats</code> on your own machine.  You can get the current version from <a href="http://github.com">Github</a> by first installing the <code>devtools</code> package from the CRAN repository, and then running the following commands in a fresh R session:</p>

<pre><code>require(devtools)
install_github(repo = "homerhanumat/tigerstats")
</code></pre>

<p>In this tutorial we will work with a couple of data sets:  <code>mtcars</code> from the <code>data</code> package that comes with the basic R installation, and <code>fuel</code> from the <code>tigerstats</code> package, so make sure you become familiar with them.  The following commands may be helpful in this regard:</p>

<pre><code>data(mtcars)
View(mtcars)
help(mtcars)
</code></pre>

<p>For <code>fuel</code>:</p>

<pre><code>data(fuel)
View(fuel)
help(fuel)
</code></pre>

<h2 id="formula-data-input">Formula-Data Input</h2>

<p>Like many R functions, <code>lmGC()</code> accepts formula-data input.  The general command looks like:</p>

<pre><code>lmGC(response ~ explanatory, data = DataFrame)
</code></pre>

<p>If you are interested in studying the relationship between the fuel efficiency of a car (<strong>mpg</strong> in the <code>mtcars</code> data frame, measured in miles per gallon) and its weight (<strong>wt</strong> in <code>mtcars</code>, measure din thousands of pounds), then you  can run:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p>The output to the console is rather minimal.  You get:</p>

<ul>
  <li>the correlation coefficient $r$.  We need to look at a scatter plot to be really sure about it (for plots, see below), but at this point the value $r = -0.87$ indicates a fairly strong negative linear relationship between fuel efficiency and weight.</li>
  <li>the equation of the regression line.  From the slope of -5.34, we see that for every thousand pound increase in the weight of a car, we predict a 5.34 mpg decrease in the fuel efficiency.</li>
  <li>the residual standard error $s$.  As a rough rule of thumb, we figure that when we use the regression equation to predict the fuel efficiency of a car from its weight, that prediction is liable to be off by about $s$ miles per gallon, or so.</li>
  <li>the unadjusted $R^2$.  We see here that about 75% of the variation in the fuel efficiency of the cars in the data set is accounted for by the variation in their weights.  since $R^2$ is fairly high, it seems that weight is a fairly decent predictor of fuel efficiency.</li>
</ul>

<h2 id="prediction">Prediction</h2>

<p>When the value of the explanatory variable for an individual is known and you wish to predict the value of the response variable for that individual, you can use the <code>predict()</code> function.  Its arguments are the linear model that is created by <code>lmGC()</code>, and the value <code>x</code> of the explanatory variable.</p>

<p>If you think that you might want to use <code>predict()</code>, you may first want to store the model in a variable, for example:</p>

<pre><code>WeightEff &lt;- lmGC(mpg ~ wt, data = mtcars)
</code></pre>

<p>Then if you want to predict the fuel efficiency of a car that weights 3000 pounds, run this command:</p>

<pre><code>predict(WeightEff, x = 3)

## [1] 21.25
</code></pre>

<p>We predict that a 3000 pound car (from the time of the Motor Trend study that produced this data) would have a fuel efficiency of 21.25 mpg, give or take about 3.046 mpg or so.  (Note how we used $s$ as a rough give-or-take figure.)</p>

<h2 id="plotting">Plotting</h2>

<p>But we are getting ahead of ourselves!  In order to use linear models at all, we need to make sure that our data really do show a linear relationship.  The single best way to check this is to look at a scatterplot, so <code>lmGC()</code> comes equipped with a option to produce one:  just set the argument <code>graph</code> to <code>TRUE</code>, as follows:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars, graph = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p><img src="/images/figure/mtcarsscatter.png" alt="Speed and fuel efficiency in the Motor Trends study" /> </p>

<p>The scatterplot includes the regression line. Indeed, the cloud of point seems to follow a line fairly well.  The relationship may be thought of as linear, so using <code>lmGC()</code> for tasks like prediction does make sense, for this data.</p>

<p>You can also perform some simple diagnostics, by setting the argument <code>diag</code> to <code>TRUE</code>:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars, diag = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p><img src="/images/figure/mtcarsdiag.png" alt="Diagnostic plots for the Motor Trends study" /> </p>

<p>You get two graphs:</p>

<ul>
  <li>a density plot of the residuals.  If the distribution of the residuals is roughly bell-shaped, then you can actually employ a ``regression’’ version of the 68-95 Rule:  when you make a prediction using the regression line, there is a about a 68% chance that your predcition will be wtthin one $s$ of the actual $y$-value, and about a 95% chance that ti will be within $2s$ of the actual $y$-value.  <em>Neat!</em></li>
  <li>A plot of the residuals vs, the <code>fitted</code> $y$-values (the $y$-coordinates of the points on the original scatterplot).  If the points exhibit a linear relationship with about the same amount of scatter all the way along the regression line, then this plot should look like a random cloud of points.  In this case, it confirms that our linear model is appropriate to the data.</li>
</ul>

<p>On the other hand, consider <code>fuel</code> data frame, which shows the results of a study where a British Ford Escort was driven at various speed along a standard course, and the fuel efficiency (in kilometers traveled per liter of fuel consumed) was recorded at each speed.  Let’s try a linear model with <strong>speed</strong> as explanatory and <strong>efficiency</strong> as the response, with some diagnostic plots attached:</p>

<pre><code>lmGC(efficiency ~ speed, data = fuel, diag = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.1716 
## 
## Equation of Regression Line:
## 
## 	 efficiency = 11.06 + -0.0147 * speed 
## 
## Residual Standard Error:	s   = 3.905 
## R^2 (unadjusted):		R^2 = 0.0295
</code></pre>

<p><img src="/images/figure/fordescortsdiag.png" alt="Diagnostic Plots for British Ford Escort Study" /> </p>

<p>The residuals have a roughly-bell-shaped distribution, but the residual plot is clearly patterned.  Something is amiss!  Let’s look at a scatterplot of the data:</p>

<pre><code>xyplot(efficiency~speed,data=fuel,
       xlab="speed (kilometers/hour",
       ylab="fuel effiency (liters/100km",
       pch=19,col="blue",type=c("p","r"),
       main="Speed and Fuel Efficiency\nfor a British Ford Escort")
</code></pre>

<p><img src="/images/figure/fordescortscatter.png" alt="Speed and Fuel Efficiency for a British Ford Escort" /> </p>

<p>The relationship is strongly curvilinear:  it makes no sense to use the regression line (shown in the plot above) to study the relationship between these two variables!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[chisq.testGC() Tutorial]]></title>
    <link href="http://homerhanumat.github.io/blog/2014/02/01/chisqtestGCtutorial/"/>
    <updated>2014-02-01T15:00:00-05:00</updated>
    <id>http://homerhanumat.github.io/blog/2014/02/01/chisqtestGCtutorial</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#formula-data-input">Formula-Data Input</a></li>
  <li><a href="#two-way-table-input">Two-Way Table Input</a></li>
  <li><a href="#a-table-from-summary-data">A Table From Summary Data</a></li>
  <li><a href="#simulation">Simulation</a>    <ul>
      <li><a href="#explanatory-tallies-fixed">Explanatory Tallies Fixed</a></li>
      <li><a href="#explanatory-tallies-random">Explanatory Tallies Random</a></li>
      <li><a href="#both-tallies-fixed">Both Tallies Fixed</a></li>
    </ul>
  </li>
  <li><a href="#graphs-of-the-p-value">Graphs of the P-Value</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p>You use the $\chi^2$-test when you are addressing the inferential aspect of a research question about the relationship between two factor variables.  That is, you want to know whether any relationship between the two variables that you might have observed in your data is real or could reasonably be explained as chance variation in the process that resulted in the data.</p>

<p>The function (and some of the data) that we will use comes from the <code>tigerstats</code> package, so make sure that it is loaded:</p>

<pre><code>require(tigerstats)
</code></pre>

<h2 id="formula-data-input">Formula-Data Input</h2>

<p>When your data are in raw form, straight from a data frame, you can perform the test using “formula-data input”.  For example, in the <code>mat111survey</code> data, we might wonder whether sex and seating preference are related, in the population from which the sample was (allegedly randomly) drawn.  The function call and the output are as follows:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey)

## Pearson's Chi-squared test 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1546
</code></pre>

<h2 id="two-way-table-input">Two-Way Table Input</h2>

<p>Sometimes you already have a two-way table on hand:</p>

<pre><code>SexSeat &lt;- xtabs(~sex + seat, data = m111survey)
SexSeat

##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
</code></pre>

<p>In that case you can save yourself some typing by entering the table in place of the formula and the <code>data</code> arguments:</p>

<pre><code>chisq.testGC(SexSeat)
</code></pre>

<h2 id="a-table-from-summary-data">A Table From Summary Data</h2>

<p>Remember:  if you are given summary data, only, then you can construct a nice two-way table and enter it into <code>chisquare.testGC()</code>.  Suppose that you want this two-way table:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">Front</th>
      <th style="text-align: center">Middle</th>
      <th style="text-align: center">Back</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Female</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: left">Male</td>
      <td style="text-align: center">8</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">7</td>
    </tr>
  </tbody>
</table>

<p>You can get it as follows:</p>

<pre><code>MySexSeat &lt;- rbind(female = c(19, 16, 5), male = c(8, 16, 7))
colnames(MySexSeat) &lt;- c("front", "middle", "back")
</code></pre>

<p>Let’s just check to see that this worked:</p>

<pre><code>MySexSeat

##        front middle back
## female    19     16    5
## male       8     16    7
</code></pre>

<p>Then you can enter <code>MySexSeat</code> into the function:</p>

<pre><code>chisq.testGC(MySexSeat)
</code></pre>

<h2 id="simulation">Simulation</h2>

<p>When the Null’s expected counts are low, <code>chisq.testGC()</code> delivers a warning and suggest the use of simulation to compute the $P$-value.  You do this by way of the argument <code>simulate.p.value</code>, and you have three options:</p>

<ul>
  <li><code>simulate.p.value = "fixed"</code></li>
  <li><code>simulate.p.value = "random"</code></li>
  <li><code>simulate.p.value = "TRUE"</code></li>
</ul>

<h3 id="explanatory-tallies-fixed">Explanatory Tallies Fixed</h3>

<p>Suppose that the objects under study are not a random sample from some larger population, and that the way chance comes into the production of the data is through random variation in all of the other factors—besides the explanatory variable— that might be associated with the response variable.  Then since the items being observed are fixed, the tally of values for the explanatory variable are fixed.  The response values for these items are the product of chance, but only through random variation in those other factors.</p>

<p>The study from the <code>ledgejump</code> data frame was an example of this.  The 21 incidents were fixed, so there were nine cold-weather incidents and 12 warm-weather incidents, no matter what.  The crowd behavior at each incident, however is still a matter of chance.</p>

<p>In such a case you might want to resample under the restriction that in all of your resamples, the tally for the explanatory variable stays just the same as it was in the data you observed.  Then your function call looks like:</p>

<pre><code>chisq.testGC(~weather + crowd.behavior, data = ledgejump, simulate.p.value = "fixed", 
    B = 2500)

## Pearson's chi-squared test with simulated p-value, fixed effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##        crowd.behavior
## weather baiting polite
##    cool       2      7
##    warm       8      4
## 
## Counts Expected by Null:
##        crowd.behavior
## weather baiting polite
##    cool    4.29   4.71
##    warm    5.71   6.29
## 
## Contributions to the chi-square statistic:
##        crowd.behavior
## weather baiting polite
##    cool    1.22   1.11
##    warm    0.91   0.83
## 
## 
## Chi-Square Statistic = 4.073 
## Degrees of Freedom of the table = 1 
## P-Value = 0.0496
</code></pre>

<p>You can set <code>B</code>, the number of resamples, as you wish, but it should be at least a few thousand.  Of course the $P$-value, having been determined by random resampling, will vary from one run of the function to another.</p>

<h3 id="explanatory-tallies-random">Explanatory Tallies Random</h3>

<p>In the <code>m111survey</code> study on sex and seating preference, the subjects are a random sample from a larger population.  In that case the tallies for both the explanatory and the response variables depend upon chance. If you simulate in such a case, then you set <code>simulate.p.value</code> to “random”:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = "random", B = 2500)

## Pearson's chi-squared test with simulated p-value, random effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1684
</code></pre>

<h3 id="both-tallies-fixed">Both Tallies Fixed</h3>

<p>If you want to resample in such a way that the tallies for BOTH the explanatory and response variables stay exactly the same as they were in the actual data, then you set <code>simulate.p.value</code> to “TRUE”.  This invokes R’s standard method for resampling:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = TRUE, B = 2500)

## Pearson's Chi-squared test with simulated p-value
## 	 (based on 2500 replicates) 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1703
</code></pre>

<p>It’s not easy to understand why R would adopt such a method, but it does have a long and honored history.  If you are ever in doubt about how to simulate, just use this third option.</p>

<h2 id="graphs-of-the-p-value">Graphs of the P-Value</h2>

<p>You can get a graph of the $P$-value in the plot window by setting the argument <code>graph</code> to TRUE.  When you did not simulate, the graph shows a density curve for the $\chi^2$ random variable with the relevant degrees of freedom.  When you simulate, the graph is a histogram of the resampled $\chi^2$-statistics.</p>

<p>Here is a case with no simulation:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, graph = TRUE)

## Pearson's Chi-squared test 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1546
</code></pre>

<p><img src="/images/figure/chisqtutnosim.png" alt="Graph of P-value, no simulation" /> </p>

<p>Here is a case with simulation:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = "random", B = 2500, 
    graph = TRUE)

## Pearson's chi-squared test with simulated p-value, random effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1572
</code></pre>

<p><img src="/images/figure/chisqtutsim.png" alt="Graph of P-value, with simulation" /> </p>

]]></content>
  </entry>
  
</feed>
