<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Student-Focused Tutorials | A Statistics Blog]]></title>
  <link href="http://homerhanumat.github.io/blog/categories/student-focused-tutorials/atom.xml" rel="self"/>
  <link href="http://homerhanumat.github.io/"/>
  <updated>2014-02-19T15:02:14-05:00</updated>
  <id>http://homerhanumat.github.io/</id>
  <author>
    <name><![CDATA[Homer White]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[lmGC() Tutorial]]></title>
    <link href="http://homerhanumat.github.io/blog/2014/02/15/lmGCtutorial/"/>
    <updated>2014-02-15T15:00:00-05:00</updated>
    <id>http://homerhanumat.github.io/blog/2014/02/15/lmGCtutorial</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#formula-data-input">Formula-Data Input</a></li>
  <li><a href="#prediction">Prediction</a></li>
  <li><a href="#plotting">Plotting</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p>The <code>lmGC()</code> is a starter-tool for simple linear regression, when you are studying the relationship between two numerical variables, one of which you consider to be an explanatory or predictor variable and the other of which you think of as the response.  It’s only a starter-tool:  by the end of the course, or in later statistics courses, you will move on to R’s function <code>lm()</code>, which allows you to work with more than one explanatory variable and which provides additional useful information in its output.  Also, in <code>lm()</code> the explanatory variable(s) can even be factors!</p>

<p>The function (and some of the data) that we will use comes from the <code>tigerstats</code> package, so make sure that it is loaded:</p>

<pre><code>require(tigerstats)
</code></pre>

<p><strong>Note:</strong>  If you are not working with the R Studio server hosted by Georgetown College, then you will need install <code>tigerstats</code> on your own machine.  You can get the current version from <a href="http://github.com">Github</a> by first installing the <code>devtools</code> package from the CRAN repository, and then running the following commands in a fresh R session:</p>

<pre><code>require(devtools)
install_github(repo = "homerhanumat/tigerstats")
</code></pre>

<p>In this tutorial we will work with a couple of data sets:  <code>mtcars</code> from the <code>data</code> package that comes with the basic R installation, and <code>fuel</code> from the <code>tigerstats</code> package, so make sure you become familiar with them.  The following commands may be helpful in this regard:</p>

<pre><code>data(mtcars)
View(mtcars)
help(mtcars)
</code></pre>

<p>For <code>fuel</code>:</p>

<pre><code>data(fuel)
View(fuel)
help(fuel)
</code></pre>

<h2 id="formula-data-input">Formula-Data Input</h2>

<p>Like many R functions, <code>lmGC()</code> accepts formula-data input.  The general command looks like:</p>

<pre><code>lmGC(response ~ explanatory, data = DataFrame)
</code></pre>

<p>If you are interested in studying the relationship between the fuel efficiency of a car (<strong>mpg</strong> in the <code>mtcars</code> data frame, measured in miles per gallon) and its weight (<strong>wt</strong> in <code>mtcars</code>, measure din thousands of pounds), then you  can run:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p>The output to the console is rather minimal.  You get:</p>

<ul>
  <li>the correlation coefficient $r$.  We need to look at a scatter plot to be really sure about it (for plots, see below), but at this point the value $r = -0.87$ indicates a fairly strong negative linear relationship between fuel efficiency and weight.</li>
  <li>the equation of the regression line.  From the slope of -5.34, we see that for every thousand pound increase in the weight of a car, we predict a 5.34 mpg decrease in the fuel efficiency.</li>
  <li>the residual standard error $s$.  As a rough rule of thumb, we figure that when we use the regression equation to predict the fuel efficiency of a car from its weight, that prediction is liable to be off by about $s$ miles per gallon, or so.</li>
  <li>the unadjusted $R^2$.  We see here that about 75% of the variation in the fuel efficiency of the cars in the data set is accounted for by the variation in their weights.  since $R^2$ is fairly high, it seems that weight is a fairly decent predictor of fuel efficiency.</li>
</ul>

<h2 id="prediction">Prediction</h2>

<p>When the value of the explanatory variable for an individual is known and you wish to predict the value of the response variable for that individual, you can use the <code>predict()</code> function.  Its arguments are the linear model that is created by <code>lmGC()</code>, and the value <code>x</code> of the explanatory variable.</p>

<p>If you think that you might want to use <code>predict()</code>, you may first want to store the model in a variable, for example:</p>

<pre><code>WeightEff &lt;- lmGC(mpg ~ wt, data = mtcars)
</code></pre>

<p>Then if you want to predict the fuel efficiency of a car that weights 3000 pounds, run this command:</p>

<pre><code>predict(WeightEff, x = 3)

## [1] 21.25
</code></pre>

<p>We predict that a 3000 pound car (from the time of the Motor Trend study that produced this data) would have a fuel efficiency of 21.25 mpg, give or take about 3.046 mpg or so.  (Note how we used $s$ as a rough give-or-take figure.)</p>

<h2 id="plotting">Plotting</h2>

<p>But we are getting ahead of ourselves!  In order to use linear models at all, we need to make sure that our data really do show a linear relationship.  The single best way to check this is to look at a scatterplot, so <code>lmGC()</code> comes equipped with a option to produce one:  just set the argument <code>graph</code> to <code>TRUE</code>, as follows:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars, graph = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p><img src="/images/figure/mtcarsscatter.png" alt="Speed and fuel efficiency in the Motor Trends study" /> </p>

<p>The scatterplot includes the regression line. Indeed, the cloud of point seems to follow a line fairly well.  The relationship may be thought of as linear, so using <code>lmGC()</code> for tasks like prediction does make sense, for this data.</p>

<p>You can also perform some simple diagnostics, by setting the argument <code>diag</code> to <code>TRUE</code>:</p>

<pre><code>lmGC(mpg ~ wt, data = mtcars, diag = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.8677 
## 
## Equation of Regression Line:
## 
## 	 mpg = 37.29 + -5.345 * wt 
## 
## Residual Standard Error:	s   = 3.046 
## R^2 (unadjusted):		R^2 = 0.7528
</code></pre>

<p><img src="/images/figure/mtcarsdiag.png" alt="Diagnostic plots for the Motor Trends study" /> </p>

<p>You get two graphs:</p>

<ul>
  <li>a density plot of the residuals.  If the distribution of the residuals is roughly bell-shaped, then you can actually employ a ``regression’’ version of the 68-95 Rule:  when you make a prediction using the regression line, there is a about a 68% chance that your predcition will be wtthin one $s$ of the actual $y$-value, and about a 95% chance that ti will be within $2s$ of the actual $y$-value.  <em>Neat!</em></li>
  <li>A plot of the residuals vs, the <code>fitted</code> $y$-values (the $y$-coordinates of the points on the original scatterplot).  If the points exhibit a linear relationship with about the same amount of scatter all the way along the regression line, then this plot should look like a random cloud of points.  In this case, it confirms that our linear model is appropriate to the data.</li>
</ul>

<p>On the other hand, consider <code>fuel</code> data frame, which shows the results of a study where a British Ford Escort was driven at various speed along a standard course, and the fuel efficiency (in kilometers traveled per liter of fuel consumed) was recorded at each speed.  Let’s try a linear model with <strong>speed</strong> as explanatory and <strong>efficiency</strong> as the response, with some diagnostic plots attached:</p>

<pre><code>lmGC(efficiency ~ speed, data = fuel, diag = TRUE)

## 
## 			Simple Linear Regression
## 
## Correlation coefficient r =  -0.1716 
## 
## Equation of Regression Line:
## 
## 	 efficiency = 11.06 + -0.0147 * speed 
## 
## Residual Standard Error:	s   = 3.905 
## R^2 (unadjusted):		R^2 = 0.0295
</code></pre>

<p><img src="/images/figure/fordescortsdiag.png" alt="Diagnostic Plots for British Ford Escort Study" /> </p>

<p>The residuals have a roughly-bell-shaped distribution, but the residual plot is clearly patterned.  Something is amiss!  Let’s look at a scatterplot of the data:</p>

<pre><code>xyplot(efficiency~speed,data=fuel,
       xlab="speed (kilometers/hour",
       ylab="fuel effiency (liters/100km",
       pch=19,col="blue",type=c("p","r"),
       main="Speed and Fuel Efficiency\nfor a British Ford Escort")
</code></pre>

<p><img src="/images/figure/fordescortscatter.png" alt="Speed and Fuel Efficiency for a British Ford Escort" /> </p>

<p>The relationship is strongly curvilinear:  it makes no sense to use the regression line (shown in the plot above) to study the relationship between these two variables!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[chisq.testGC() Tutorial]]></title>
    <link href="http://homerhanumat.github.io/blog/2014/02/01/chisqtestGCtutorial/"/>
    <updated>2014-02-01T15:00:00-05:00</updated>
    <id>http://homerhanumat.github.io/blog/2014/02/01/chisqtestGCtutorial</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#formula-data-input">Formula-Data Input</a></li>
  <li><a href="#two-way-table-input">Two-Way Table Input</a></li>
  <li><a href="#a-table-from-summary-data">A Table From Summary Data</a></li>
  <li><a href="#simulation">Simulation</a>    <ul>
      <li><a href="#explanatory-tallies-fixed">Explanatory Tallies Fixed</a></li>
      <li><a href="#explanatory-tallies-random">Explanatory Tallies Random</a></li>
      <li><a href="#both-tallies-fixed">Both Tallies Fixed</a></li>
    </ul>
  </li>
  <li><a href="#graphs-of-the-p-value">Graphs of the P-Value</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<p>You use the $\chi^2$-test when you are addressing the inferential aspect of a research question about the relationship between two factor variables.  That is, you want to know whether any relationship between the two variables that you might have observed in your data is real or could reasonably be explained as chance variation in the process that resulted in the data.</p>

<p>The function (and some of the data) that we will use comes from the <code>tigerstats</code> package, so make sure that it is loaded:</p>

<pre><code>require(tigerstats)
</code></pre>

<h2 id="formula-data-input">Formula-Data Input</h2>

<p>When your data are in raw form, straight from a data frame, you can perform the test using “formula-data input”.  For example, in the <code>mat111survey</code> data, we might wonder whether sex and seating preference are related, in the population from which the sample was (allegedly randomly) drawn.  The function call and the output are as follows:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey)

## Pearson's Chi-squared test 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1546
</code></pre>

<h2 id="two-way-table-input">Two-Way Table Input</h2>

<p>Sometimes you already have a two-way table on hand:</p>

<pre><code>SexSeat &lt;- xtabs(~sex + seat, data = m111survey)
SexSeat

##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
</code></pre>

<p>In that case you can save yourself some typing by entering the table in place of the formula and the <code>data</code> arguments:</p>

<pre><code>chisq.testGC(SexSeat)
</code></pre>

<h2 id="a-table-from-summary-data">A Table From Summary Data</h2>

<p>Remember:  if you are given summary data, only, then you can construct a nice two-way table and enter it into <code>chisquare.testGC()</code>.  Suppose that you want this two-way table:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">Front</th>
      <th style="text-align: center">Middle</th>
      <th style="text-align: center">Back</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Female</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">5</td>
    </tr>
    <tr>
      <td style="text-align: left">Male</td>
      <td style="text-align: center">8</td>
      <td style="text-align: center">16</td>
      <td style="text-align: center">7</td>
    </tr>
  </tbody>
</table>

<p>You can get it as follows:</p>

<pre><code>MySexSeat &lt;- rbind(female = c(19, 16, 5), male = c(8, 16, 7))
colnames(MySexSeat) &lt;- c("front", "middle", "back")
</code></pre>

<p>Let’s just check to see that this worked:</p>

<pre><code>MySexSeat

##        front middle back
## female    19     16    5
## male       8     16    7
</code></pre>

<p>Then you can enter <code>MySexSeat</code> into the function:</p>

<pre><code>chisq.testGC(MySexSeat)
</code></pre>

<h2 id="simulation">Simulation</h2>

<p>When the Null’s expected counts are low, <code>chisq.testGC()</code> delivers a warning and suggest the use of simulation to compute the $P$-value.  You do this by way of the argument <code>simulate.p.value</code>, and you have three options:</p>

<ul>
  <li><code>simulate.p.value = "fixed"</code></li>
  <li><code>simulate.p.value = "random"</code></li>
  <li><code>simulate.p.value = "TRUE"</code></li>
</ul>

<h3 id="explanatory-tallies-fixed">Explanatory Tallies Fixed</h3>

<p>Suppose that the objects under study are not a random sample from some larger population, and that the way chance comes into the production of the data is through random variation in all of the other factors—besides the explanatory variable— that might be associated with the response variable.  Then since the items being observed are fixed, the tally of values for the explanatory variable are fixed.  The response values for these items are the product of chance, but only through random variation in those other factors.</p>

<p>The study from the <code>ledgejump</code> data frame was an example of this.  The 21 incidents were fixed, so there were nine cold-weather incidents and 12 warm-weather incidents, no matter what.  The crowd behavior at each incident, however is still a matter of chance.</p>

<p>In such a case you might want to resample under the restriction that in all of your resamples, the tally for the explanatory variable stays just the same as it was in the data you observed.  Then your function call looks like:</p>

<pre><code>chisq.testGC(~weather + crowd.behavior, data = ledgejump, simulate.p.value = "fixed", 
    B = 2500)

## Pearson's chi-squared test with simulated p-value, fixed effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##        crowd.behavior
## weather baiting polite
##    cool       2      7
##    warm       8      4
## 
## Counts Expected by Null:
##        crowd.behavior
## weather baiting polite
##    cool    4.29   4.71
##    warm    5.71   6.29
## 
## Contributions to the chi-square statistic:
##        crowd.behavior
## weather baiting polite
##    cool    1.22   1.11
##    warm    0.91   0.83
## 
## 
## Chi-Square Statistic = 4.073 
## Degrees of Freedom of the table = 1 
## P-Value = 0.0496
</code></pre>

<p>You can set <code>B</code>, the number of resamples, as you wish, but it should be at least a few thousand.  Of course the $P$-value, having been determined by random resampling, will vary from one run of the function to another.</p>

<h3 id="explanatory-tallies-random">Explanatory Tallies Random</h3>

<p>In the <code>m111survey</code> study on sex and seating preference, the subjects are a random sample from a larger population.  In that case the tallies for both the explanatory and the response variables depend upon chance. If you simulate in such a case, then you set <code>simulate.p.value</code> to “random”:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = "random", B = 2500)

## Pearson's chi-squared test with simulated p-value, random effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1684
</code></pre>

<h3 id="both-tallies-fixed">Both Tallies Fixed</h3>

<p>If you want to resample in such a way that the tallies for BOTH the explanatory and response variables stay exactly the same as they were in the actual data, then you set <code>simulate.p.value</code> to “TRUE”.  This invokes R’s standard method for resampling:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = TRUE, B = 2500)

## Pearson's Chi-squared test with simulated p-value
## 	 (based on 2500 replicates) 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1703
</code></pre>

<p>It’s not easy to understand why R would adopt such a method, but it does have a long and honored history.  If you are ever in doubt about how to simulate, just use this third option.</p>

<h2 id="graphs-of-the-p-value">Graphs of the P-Value</h2>

<p>You can get a graph of the $P$-value in the plot window by setting the argument <code>graph</code> to TRUE.  When you did not simulate, the graph shows a density curve for the $\chi^2$ random variable with the relevant degrees of freedom.  When you simulate, the graph is a histogram of the resampled $\chi^2$-statistics.</p>

<p>Here is a case with no simulation:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, graph = TRUE)

## Pearson's Chi-squared test 
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1546
</code></pre>

<p><img src="/images/figure/chisqtutnosim.png" alt="Graph of P-value, no simulation" /> </p>

<p>Here is a case with simulation:</p>

<pre><code>chisq.testGC(~sex + seat, data = m111survey, simulate.p.value = "random", B = 2500, 
    graph = TRUE)

## Pearson's chi-squared test with simulated p-value, random effects
## 	 (based on 2500 resamples)
## 
## Observed Counts:
##         seat
## sex      1_front 2_middle 3_back
##   female      19       16      5
##   male         8       16      7
## 
## Counts Expected by Null:
##         seat
## sex      1_front 2_middle 3_back
##   female   15.21    18.03   6.76
##   male     11.79    13.97   5.24
## 
## Contributions to the chi-square statistic:
##         seat
## sex      1_front 2_middle 3_back
##   female    0.94     0.23   0.46
##   male      1.22     0.29   0.59
## 
## 
## Chi-Square Statistic = 3.734 
## Degrees of Freedom of the table = 2 
## P-Value = 0.1572
</code></pre>

<p><img src="/images/figure/chisqtutsim.png" alt="Graph of P-value, with simulation" /> </p>

]]></content>
  </entry>
  
</feed>
